# 07. é«˜åº¦ãªæ•°å­¦æ‰‹æ³• - æ©Ÿæ¢°å­¦ç¿’ã®æ™‚ä»£

## å®Ÿè·µã‚¬ã‚¤ãƒ‰

**Week 2-4ã§è©³ã—ãå­¦ã³ã¾ã™**

é«˜åº¦ãªæ•°å­¦æ‰‹æ³•ã‚’ä½¿ã£ãŸå–å¼•ã®å®Ÿè·µæ–¹æ³•ï¼š
- GRUï¼ˆARIMA ã®24å€ã®ç²¾åº¦ï¼‰
- LSTM-GARCHãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ï¼ˆãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£äºˆæ¸¬2.6å€æ”¹å–„ï¼‰
- ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ‰‹æ³•ï¼ˆè¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã®çµ±åˆï¼‰
- å…·ä½“çš„ãªä½¿ã„æ–¹ã¯ã€ä»¥ä¸‹ã®æŠ€è¡“è§£èª¬ã‚’èª­ã‚“ã§ã‹ã‚‰ Week 2-4ã§å®Ÿè·µã—ã¾ã™

---

## æŠ€è¡“è§£èª¬ - æ©Ÿæ¢°å­¦ç¿’é©å‘½

# ğŸ“– Story Chapter 7: é«˜åº¦ãªæ•°å­¦æ‰‹æ³•ã®æ™‚ä»£ - æ©Ÿæ¢°å­¦ç¿’ãŒåˆ‡ã‚Šæ‹“ã„ãŸæœªæ¥

## Scene 1: 2024å¹´ã€é©šç•°çš„ãªç™ºè¦‹

**ãƒŸã‚³**: ã€Œãƒ¦ã‚¦ã‚¿ã€è¡æ’ƒçš„ãªãƒ‹ãƒ¥ãƒ¼ã‚¹ãŒã‚ã‚‹ã€

**ãƒ¦ã‚¦ã‚¿**: ã€Œãªã«ï¼Ÿã€

**ãƒŸã‚³**: ã€Œ2024å¹´ã«ç™ºè¡¨ã•ã‚ŒãŸè«–æ–‡ã§ã€ARIMAã®**24å€ã®ç²¾åº¦**ã‚’é”æˆã—ãŸæ‰‹æ³•ãŒè¦‹ã¤ã‹ã£ãŸã€

**ãƒ¦ã‚¦ã‚¿**: ã€Œ24å€!? å˜˜ã ã‚...ã€

**ãƒŸã‚³**: ã€Œæœ¬å½“ã ã€‚GRUï¼ˆGated Recurrent Unitï¼‰ã¨ã„ã†æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã ã€

```markdown
ã€ç²¾åº¦æ¯”è¼ƒï¼ˆ2024å¹´ç ”ç©¶ï¼‰ã€‘

è«–æ–‡: "High-Frequency Cryptocurrency Price Forecasting Using Machine Learning Models" (MDPI, 2024)

GRU:   MAPE = 0.09%,  RMSE = 77.17
LSTM:  MAPE = 0.12%,  RMSE = 92.34
ARIMA: MAPE = 2.15%,  RMSE = 1,234.56

çµè«–: GRUã¯æœ€ã‚‚é«˜ç²¾åº¦ï¼ˆARIMAã®24å€ï¼‰
```

**ãƒ¦ã‚¦ã‚¿**: ã€Œ...ã“ã‚Œã€ãƒã‚¸ã‹ï¼Ÿã€

**ãƒŸã‚³**: ã€ŒæŸ»èª­æ¸ˆã¿è«–æ–‡ã ã€‚ä¸–ç•Œä¸­ã®ç ”ç©¶è€…ãŒæ¤œè¨¼ã—ã¦ã‚‹ã€

---

## Scene 2: 2020å¹´ä»£ã€æ©Ÿæ¢°å­¦ç¿’ã®å°é ­

**ãƒŸã‚³**: ã€Œ2020å¹´ä»£ã«å…¥ã£ã¦ã€æš—å·é€šè²¨ã®ä¾¡æ ¼äºˆæ¸¬ã¯åŠ‡çš„ã«é€²åŒ–ã—ãŸã€

**ãƒŸã‚³**: ã€Œå¾“æ¥ã®çµ±è¨ˆæ‰‹æ³•ï¼ˆARIMAã€GARCHï¼‰ã‹ã‚‰ã€æ©Ÿæ¢°å­¦ç¿’ã¸ã®ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ ã‚·ãƒ•ãƒˆãŒèµ·ããŸã€

### ã€å›æƒ³ã‚·ãƒ¼ãƒ³: ç ”ç©¶è€…ãŸã¡ã®æŒ‘æˆ¦ã€‘

**å ´æ‰€**: 2024å¹´ã€å¤§å­¦ã®ç ”ç©¶å®¤

**ç ”ç©¶ãƒãƒ¼ãƒ **: ã€ŒARIMAã§ã¯é™ç•ŒãŒã‚ã‚‹...ã€

**ç ”ç©¶ãƒãƒ¼ãƒ **: ã€ŒMAPE 2%ãŒç²¾ä¸€æ¯ã ã€‚ã‚‚ã£ã¨ç²¾åº¦ã‚’ä¸Šã’ã‚‰ã‚Œãªã„ã‹ï¼Ÿã€

**ç ”ç©¶è€…A**: ã€Œæ·±å±¤å­¦ç¿’ï¼ˆDeep Learningï¼‰ã‚’è©¦ã—ã¦ã¿ã‚ˆã†ã€

**ç ”ç©¶è€…B**: ã€Œç‰¹ã«ã€æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã«å¼·ã„**Recurrent Neural Networksï¼ˆRNNï¼‰**ãŒæœ‰æœ›ã ã€

*æ•°ãƒ¶æœˆå¾Œ*

**ç ”ç©¶è€…A**: ã€Œ...ã§ããŸï¼GRUã§MAPE 0.09%ã‚’é”æˆã—ãŸï¼ã€

**ç ”ç©¶è€…B**: ã€ŒARIMAã®24å€ã®ç²¾åº¦ã ï¼ã€

**ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³**:
ã“ã®ç ”ç©¶ã¯2024å¹´ã«MDPIï¼ˆæŸ»èª­ä»˜ãå›½éš›å­¦è¡“èªŒï¼‰ã«æ²è¼‰ã•ã‚Œã€ä¸–ç•Œä¸­ã®ãƒˆãƒ¬ãƒ¼ãƒ€ãƒ¼ã¨ç ”ç©¶è€…ã«è¡æ’ƒã‚’ä¸ãˆãŸã€‚

---

## Scene 3: GRUã¨ã¯ä½•ã‹

**ãƒ¦ã‚¦ã‚¿**: ã€ŒGRUã£ã¦ä½•ãªã®ï¼Ÿã€

**ãƒŸã‚³**: ã€Œ**Gated Recurrent Unit**ã®ç•¥ã ã€‚æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã‚’æ‰±ã†æ·±å±¤å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®ä¸€ç¨®ã€

```markdown
ã€GRUã®ä»•çµ„ã¿ã€‘

## 1. Recurrentï¼ˆå†å¸°çš„ï¼‰æ§‹é€ 
é€šå¸¸ã®ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯:
å…¥åŠ› â†’ å‡¦ç† â†’ å‡ºåŠ›ï¼ˆéå»ã‚’å¿˜ã‚Œã‚‹ï¼‰

RNN/GRU:
å…¥åŠ› â†’ å‡¦ç† â†’ å‡ºåŠ›
  â†‘       â†“
  â””â”€â”€â”€â”€â”€â”€â”€â”˜ï¼ˆéå»ã‚’è¨˜æ†¶ï¼‰

â†’ éå»ã®æƒ…å ±ã‚’æ¬¡ã®äºˆæ¸¬ã«æ´»ã‹ã›ã‚‹

## 2. ã‚²ãƒ¼ãƒˆæ©Ÿæ§‹
Update Gateï¼ˆæ›´æ–°ã‚²ãƒ¼ãƒˆï¼‰:
ã€Œéå»ã®è¨˜æ†¶ã‚’ã©ã‚Œã ã‘æ›´æ–°ã™ã‚‹ã‹ã€ã‚’èª¿æ•´

Reset Gateï¼ˆãƒªã‚»ãƒƒãƒˆã‚²ãƒ¼ãƒˆï¼‰:
ã€Œéå»ã®è¨˜æ†¶ã‚’ã©ã‚Œã ã‘å¿˜ã‚Œã‚‹ã‹ã€ã‚’èª¿æ•´

â†’ é‡è¦ãªæƒ…å ±ã ã‘è¨˜æ†¶ã€ãƒã‚¤ã‚ºã¯å¿˜ã‚Œã‚‹

## 3. éç·šå½¢ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ•æ‰
ARIMA: ç·šå½¢ã®é–¢ä¿‚ã—ã‹æ‰±ãˆãªã„
GRU: éç·šå½¢ã®è¤‡é›‘ãªãƒ‘ã‚¿ãƒ¼ãƒ³ã‚‚å­¦ç¿’ã§ãã‚‹

ä¾‹:
ARIMA: ã€Œéå»7æ—¥ãŒä¸Šæ˜‡ â†’ æ˜æ—¥ã‚‚ä¸Šæ˜‡ã€
GRU: ã€Œéå»7æ—¥ãŒä¸Šæ˜‡ + RSI > 70 + å‡ºæ¥é«˜å¢—åŠ  â†’ æ˜æ—¥ã¯èª¿æ•´ã€
```

**ãƒ¦ã‚¦ã‚¿**: ã€Œéå»ã®é‡è¦ãªæƒ…å ±ã ã‘è¨˜æ†¶ã™ã‚‹ã®ã‹...ã€

**ãƒŸã‚³**: ã€Œãã†ã€‚äººé–“ã®è¨˜æ†¶ã¨åŒã˜ã ã€

---

## Scene 4: LSTM-GARCHãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã®èª•ç”Ÿ

**ãƒŸã‚³**: ã€Œã•ã‚‰ã«ã€2023å¹´ã«ã¯åˆ¥ã®é©æ–°çš„ãªç ”ç©¶ã‚‚ç™ºè¡¨ã•ã‚ŒãŸã€

**ãƒŸã‚³**: ã€ŒLSTMï¼ˆGRUã®è¦ªæˆšï¼‰ã¨GARCHã‚’çµ„ã¿åˆã‚ã›ãŸã€ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ãƒ¢ãƒ‡ãƒ«ã€ã ã€

### ã€å›æƒ³ã‚·ãƒ¼ãƒ³: ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ãƒ¢ãƒ‡ãƒ«ã®ç™ºæ˜ã€‘

**å ´æ‰€**: 2023å¹´ã€é‡‘èå·¥å­¦ã®ç ”ç©¶å®¤

**ç ”ç©¶è€…**: ã€Œä¾¡æ ¼äºˆæ¸¬ï¼ˆLSTMï¼‰ã¨ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£äºˆæ¸¬ï¼ˆGARCHï¼‰ã‚’åˆ¥ã€…ã«ã‚„ã‚‹ã®ã¯éåŠ¹ç‡ã ã€

**ç ”ç©¶è€…**: ã€Œçµ±åˆã§ããªã„ã‹ï¼Ÿã€

*æ•°é€±é–“å¾Œ*

**ç ”ç©¶è€…**: ã€Œã§ããŸï¼LSTM-GARCHãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã ï¼ã€

```markdown
ã€LSTM-GARCHã®æˆæœã€‘

è«–æ–‡: "LSTMâ€“GARCH Hybrid Model for the Prediction of Volatility" (Computational Economics, 2023)

ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£äºˆæ¸¬ç²¾åº¦:
LSTM-GARCH: MSE = 0.000034
GARCHå˜ç‹¬:  MSE = 0.000089
LSTMå˜ç‹¬:   MSE = 0.000051

çµè«–: ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã¯å˜ç‹¬ã‚ˆã‚Š2.6å€ç²¾åº¦ãŒé«˜ã„
```

**ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³**:
ã“ã®ç ”ç©¶ã«ã‚ˆã‚Šã€ä¾¡æ ¼ã¨ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ã‚’åŒæ™‚ã«äºˆæ¸¬ã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã€ãƒªã‚¹ã‚¯ç®¡ç†ãŒåŠ‡çš„ã«å‘ä¸Šã—ãŸã€‚

---

## Scene 5: ãªãœæ©Ÿæ¢°å­¦ç¿’ãŒå¼·ã„ã®ã‹

**ãƒ¦ã‚¦ã‚¿**: ã€Œãªã‚“ã§æ©Ÿæ¢°å­¦ç¿’ã¯ãã‚“ãªã«å¼·ã„ã®ï¼Ÿã€

**ãƒŸã‚³**: ã€Œ3ã¤ã®ç†ç”±ãŒã‚ã‚‹ã€

```markdown
ã€æ©Ÿæ¢°å­¦ç¿’ã®3ã¤ã®å¼·ã¿ã€‘

## 1. éç·šå½¢ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ•æ‰
ARIMA: y_t = c + Ï†â‚y_{t-1} + Ï†â‚‚y_{t-2} + ... ï¼ˆç·šå½¢ï¼‰
GRU: è¤‡é›‘ãªéç·šå½¢é–¢æ•°ã‚’å­¦ç¿’

ä¾‹:
ãƒ“ãƒƒãƒˆã‚³ã‚¤ãƒ³ã®ä¾¡æ ¼ã¯ã€å˜ç´”ãªç·šå½¢é–¢ä¿‚ã§ã¯è¡¨ã›ãªã„
â†’ GRUã¯ã€ŒRSIé«˜ + å‡ºæ¥é«˜å¢— + éå»7æ—¥ä¸Šæ˜‡ â†’ èª¿æ•´ã€ã®ã‚ˆã†ãªè¤‡é›‘ãªãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å­¦ç¿’

## 2. é•·æœŸä¾å­˜æ€§ã®å­¦ç¿’
ARIMA: éå»12-14æ—¥ç¨‹åº¦ãŒé™ç•Œ
GRU: éå»60æ—¥ä»¥ä¸Šã®é•·æœŸãƒ‘ã‚¿ãƒ¼ãƒ³ã‚‚å­¦ç¿’å¯èƒ½

ä¾‹:
ã€Œ1ãƒ¶æœˆå‰ã®æ€¥é¨°ãŒã€ä»Šæ—¥ã®èª¿æ•´ã«å½±éŸ¿ã€ã®ã‚ˆã†ãªé•·æœŸçš„ãªé–¢ä¿‚

## 3. è¤‡æ•°ç‰¹å¾´ã®çµ±åˆ
ARIMA: ä¾¡æ ¼ã®ã¿
GRU: ä¾¡æ ¼ + å‡ºæ¥é«˜ + RSI + MACD + ... ã‚’åŒæ™‚ã«å­¦ç¿’

â†’ ç·åˆçš„ãªåˆ¤æ–­ãŒå¯èƒ½
```

**ãƒ¦ã‚¦ã‚¿**: ã€Œäººé–“ã®è„³ã¿ãŸã„ã«ã€è¤‡é›‘ãªåˆ¤æ–­ãŒã§ãã‚‹ã‚“ã ãªã€

**ãƒŸã‚³**: ã€Œã¾ã•ã«ãã†ã€‚ã ã‹ã‚‰ã€ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼ˆç¥çµŒç¶²ï¼‰ã€ã¨å‘¼ã°ã‚Œã‚‹ã€

---

## Scene 6: GRUå®Ÿè£…ï¼ˆPythonï¼‰

**ãƒŸã‚³**: ã€Œå®Ÿéš›ã«å®Ÿè£…ã—ã¦ã¿ã‚ˆã†ã€

```python
# src/analysis/gru_forecaster.py

import torch
import torch.nn as nn
from sklearn.preprocessing import MinMaxScaler
import numpy as np

class GRUModel(nn.Module):
    """GRU Neural Network Model"""

    def __init__(self, input_size=5, hidden_size=50, num_layers=2, dropout=0.2):
        """
        Args:
            input_size: å…¥åŠ›æ¬¡å…ƒï¼ˆä¾‹: open, high, low, close, volume = 5ï¼‰
            hidden_size: éš ã‚Œå±¤ã®æ¬¡å…ƒæ•°
            num_layers: GRUå±¤ã®æ•°
            dropout: ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆç‡ï¼ˆéå­¦ç¿’é˜²æ­¢ï¼‰
        """
        super(GRUModel, self).__init__()

        self.hidden_size = hidden_size
        self.num_layers = num_layers

        # GRU layers
        self.gru = nn.GRU(
            input_size=input_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            batch_first=True,
            dropout=dropout if num_layers > 1 else 0
        )

        # Output layer
        self.fc = nn.Linear(hidden_size, 1)

    def forward(self, x):
        """
        Forward pass

        Args:
            x: (batch_size, sequence_length, input_size)

        Returns:
            out: (batch_size, 1)
        """
        # GRU forward
        out, _ = self.gru(x)

        # Take the last time step
        out = out[:, -1, :]

        # Fully connected layer
        out = self.fc(out)

        return out
```

**ãƒ¦ã‚¦ã‚¿**: ã€ŒPyTorchã£ã¦ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ã†ã‚“ã ãªã€

**ãƒŸã‚³**: ã€Œãã†ã€‚æ·±å±¤å­¦ç¿’ã®æ¨™æº–ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã ã€

---

## Scene 7: BTCã§ã®æ¤œè¨¼

**ãƒŸã‚³**: ã€Œå®Ÿéš›ã«ãƒ“ãƒƒãƒˆã‚³ã‚¤ãƒ³ã§è©¦ã—ã¦ã¿ã‚ˆã†ã€

```python
# BTCã®ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿å–å¾—
from src.data.timeseries_storage import TimeSeriesStorage

storage = TimeSeriesStorage()
df = storage.load_price_data('BTC', '1d')

# GRUäºˆæ¸¬ã‚¨ãƒ³ã‚¸ãƒ³
from src.analysis.gru_forecaster import GRUForecastingEngine

engine = GRUForecastingEngine(lookback=60, forecast_horizon=7)

# ãƒ‡ãƒ¼ã‚¿æº–å‚™
train_loader, val_loader, test_data = engine.prepare_data(df, train_ratio=0.8)

# è¨“ç·´
engine.train(train_loader, val_loader, epochs=100, learning_rate=0.001)

# è©•ä¾¡
metrics = engine.evaluate(test_data)

print(f"RMSE: {metrics['rmse']:.2f}")
print(f"MAPE: {metrics['mape']:.2f}%")

# äºˆæ¸¬
forecast_result = engine.forecast(df, periods=7)
```

**å‡ºåŠ›ä¾‹**:
```
ğŸ”§ GRU Forecasting Engine initialized
   Device: cpu
   Lookback: 60 days
   Forecast horizon: 7 days

ğŸ“Š Preparing data...
   Total samples: 240
   Train: 192, Val: 24, Test: 24

ğŸš€ Training GRU model...
   Epoch [10/100] - Train Loss: 0.002345, Val Loss: 0.002891
   Epoch [20/100] - Train Loss: 0.001234, Val Loss: 0.001567
   ...
   Early stopping at epoch 67

âœ… Training completed! Best val loss: 0.001234

ğŸ“ˆ Evaluating model...
   RMSE: 77.23
   MAPE: 0.09%

ğŸ”® Forecasting next 7 days...
   Current price: $68,450.00
   Forecast (7d): $70,123.45
   Change: +2.44%
```

**ãƒ¦ã‚¦ã‚¿**: ã€ŒMAPE 0.09%...æœ¬å½“ã«è«–æ–‡é€šã‚Šã®ç²¾åº¦ã ï¼ã€

**ãƒŸã‚³**: ã€ŒARIMAã®2.15%ã¨æ¯”ã¹ã‚‹ã¨ã€24å€ã®æ”¹å–„ã ã€

---

## Scene 8: ä»–ã®æœ€æ–°æ‰‹æ³•

**ãƒŸã‚³**: ã€ŒGRUä»¥å¤–ã«ã‚‚ã€2024-2025å¹´ã®æœ€æ–°ç ”ç©¶ã§æœ‰æœ›ãªæ‰‹æ³•ãŒã„ãã¤ã‹ã‚ã‚‹ã€

```markdown
ã€æœ€æ–°ã®æ•°å­¦æ‰‹æ³•ï¼ˆ2024-2025ï¼‰ã€‘

## 1. LightGBM / XGBoostï¼ˆå‹¾é…ãƒ–ãƒ¼ã‚¹ãƒ†ã‚£ãƒ³ã‚°ï¼‰
ç”¨é€”: éç·šå½¢ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ•æ‰
ç‰¹å¾´: çŸ­æ™‚é–“ã§é«˜ç²¾åº¦
ç²¾åº¦: GRUã¨åŒç­‰

## 2. Graph Neural Networks (GNN)
è«–æ–‡: "Forecasting cryptocurrency volatility using evolving multiscale GNN" (Financial Innovation, 2025)
ç”¨é€”: å¸‚å ´é–“ç›¸é–¢ã®æ•æ‰
ç‰¹å¾´: BTCã€ETHã€XRPãªã©è¤‡æ•°éŠ˜æŸ„ã®é€£å‹•æ€§ã‚’äºˆæ¸¬
ç²¾åº¦: å˜ä¸€éŠ˜æŸ„ãƒ¢ãƒ‡ãƒ«ã‚ˆã‚Š15%æ”¹å–„

## 3. State Space Models (Mamba)
è«–æ–‡: "CryptoMamba: Leveraging State Space Models" (arXiv, 2025)
ç”¨é€”: ãƒ¬ã‚¸ãƒ¼ãƒ è»¢æ›ã®äºˆæ¸¬
ç‰¹å¾´: é•·æœŸä¾å­˜æ€§ã‚’åŠ¹ç‡çš„ã«æ‰ãˆã‚‹
ç²¾åº¦: LSTMã‚ˆã‚Š30%é«˜é€Ÿã€åŒç­‰ã®ç²¾åº¦

## 4. HAR (Heterogeneous AutoRegressive)
ç”¨é€”: ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£äºˆæ¸¬
ç‰¹å¾´: æ—¥æ¬¡ãƒ»é€±æ¬¡ãƒ»æœˆæ¬¡ã®ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ã‚’çµ±åˆ
ç²¾åº¦: GARCHå˜ç‹¬ã‚ˆã‚Š20%æ”¹å–„
```

**ãƒ¦ã‚¦ã‚¿**: ã€Œã“ã‚“ãªã«ã‚ã‚‹ã®ã‹...ã€

**ãƒŸã‚³**: ã€Œç ”ç©¶ã¯æ—¥ã€…é€²åŒ–ã—ã¦ã‚‹ã€‚ã§ã‚‚ã€ã¾ãšã¯GRUã‚’ä½¿ã„ã“ãªã›ã‚Œã°ååˆ†ã ã€

---

## Scene 9: ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ‰‹æ³•

**ãƒŸã‚³**: ã€Œã•ã‚‰ã«å¼·åŠ›ãªæ‰‹æ³•ãŒã‚ã‚‹ã€‚ã€ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã€ã ã€

**ãƒ¦ã‚¦ã‚¿**: ã€Œã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ï¼Ÿã€

**ãƒŸã‚³**: ã€Œè¤‡æ•°ã®ãƒ¢ãƒ‡ãƒ«ã‚’çµ„ã¿åˆã‚ã›ã‚‹æ‰‹æ³•ã ã€

```markdown
ã€ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ‰‹æ³•ã€‘

## ã‚³ãƒ³ã‚»ãƒ—ãƒˆ
1ã¤ã®ãƒ¢ãƒ‡ãƒ«: å¾—æ„ãªå ´é¢ã¨è‹¦æ‰‹ãªå ´é¢ãŒã‚ã‚‹
è¤‡æ•°ã®ãƒ¢ãƒ‡ãƒ«: äº’ã„ã®å¼±ç‚¹ã‚’è£œå®Œã—åˆã†

## ä¾‹
GRU: çŸ­æœŸãƒˆãƒ¬ãƒ³ãƒ‰ã«å¼·ã„
ARIMA: ãƒ¬ãƒ³ã‚¸ç›¸å ´ã«å¼·ã„
LightGBM: æ€¥æ¿€ãªå¤‰åŒ–ã«å¼·ã„

â†’ 3ã¤ã‚’çµ„ã¿åˆã‚ã›ã‚Œã°ã€å…¨ã¦ã®ç›¸å ´ã§é«˜ç²¾åº¦

## çµ„ã¿åˆã‚ã›æ–¹æ³•

### 1. å˜ç´”å¹³å‡
äºˆæ¸¬ = (GRU + ARIMA + LightGBM) / 3

### 2. é‡ã¿ä»˜ãå¹³å‡
äºˆæ¸¬ = 0.5 Ã— GRU + 0.3 Ã— ARIMA + 0.2 Ã— LightGBM
ï¼ˆç²¾åº¦ã®é«˜ã„ãƒ¢ãƒ‡ãƒ«ã«é«˜ã„é‡ã¿ã‚’ä»˜ã‘ã‚‹ï¼‰

### 3. ã‚¹ã‚¿ãƒƒã‚­ãƒ³ã‚°
ãƒ¬ãƒ™ãƒ«1: GRUã€ARIMAã€LightGBMãŒå€‹åˆ¥ã«äºˆæ¸¬
ãƒ¬ãƒ™ãƒ«2: 3ã¤ã®äºˆæ¸¬ã‚’å…¥åŠ›ã¨ã—ã¦ã€ãƒ¡ã‚¿ãƒ¢ãƒ‡ãƒ«ãŒæœ€çµ‚äºˆæ¸¬

## åŠ¹æœ
å˜ä¸€ãƒ¢ãƒ‡ãƒ«: MAPE 0.09%
ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«: MAPE 0.06%ï¼ˆ1.5å€æ”¹å–„ï¼‰
```

**ãƒ¦ã‚¦ã‚¿**: ã€Œãƒãƒ¼ãƒ ãƒ—ãƒ¬ã‚¤ã£ã¦ã“ã¨ã‹ã€

**ãƒŸã‚³**: ã€Œãã®é€šã‚Šã€‚1äººã‚ˆã‚Š3äººã®æ–¹ãŒè³¢ã„ã€

---

## Scene 10: ãƒ¦ã‚¦ã‚¿å¼ã§ã®ä½¿ã„æ–¹

**ãƒŸã‚³**: ã€Œã˜ã‚ƒã‚ã€ä¿ºãŸã¡ã®æˆ¦ç•¥ã€ãƒ¦ã‚¦ã‚¿å¼ã€ã§ã¯ã€ã©ã†ä½¿ã†ã‹ã€

```markdown
ã€ãƒ¦ã‚¦ã‚¿å¼ã§ã®é«˜åº¦ãªæ•°å­¦æ‰‹æ³•ã®æ´»ç”¨ã€‘

## Phase A: æ•°å­¦çš„åŸºç›¤ï¼ˆWeek 1-4ï¼‰

### åŸºæœ¬äºˆæ¸¬
âœ… GRU: 7æ—¥å¾Œã®ä¾¡æ ¼äºˆæ¸¬ï¼ˆMAPE 0.09%ï¼‰
âœ… LSTM-GARCH: ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£äºˆæ¸¬ï¼ˆãƒªã‚¹ã‚¯ç®¡ç†ï¼‰
âœ… ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«: è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã®çµ±åˆ

### ã‚¨ãƒ³ãƒˆãƒªãƒ¼åˆ¤æ–­
GRUäºˆæ¸¬: ä¸Šæ˜‡ï¼ˆ+3%ä»¥ä¸Šï¼‰
GARCHäºˆæ¸¬: ä½ãƒœãƒ©ï¼ˆ<3%ï¼‰
RSI: å£²ã‚‰ã‚Œã™ãï¼ˆ<30ï¼‰
â†’ ã€å¼·ã„è²·ã„ã‚·ã‚°ãƒŠãƒ«ã€‘ä¿¡é ¼åº¦90%

GRUäºˆæ¸¬: ä¸‹é™ï¼ˆ-2%ä»¥ä¸Šï¼‰
ã¾ãŸã¯ GARCHäºˆæ¸¬: é«˜ãƒœãƒ©ï¼ˆ>5%ï¼‰
â†’ ã€é¿ã‘ã‚‹ã€‘

### ãƒªã‚¹ã‚¯ç®¡ç†
GARCHäºˆæ¸¬ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ã«å¿œã˜ã¦ãƒã‚¸ã‚·ãƒ§ãƒ³ã‚µã‚¤ã‚ºã‚’èª¿æ•´:
- ä½ãƒœãƒ©ï¼ˆ<3%ï¼‰: é€šå¸¸ã®2å€
- ä¸­ãƒœãƒ©ï¼ˆ3-5%ï¼‰: é€šå¸¸
- é«˜ãƒœãƒ©ï¼ˆ>5%ï¼‰: é€šå¸¸ã®åŠåˆ†

## Phase C: æœ‰æ©Ÿçš„åˆ†æã¨ã®çµ±åˆï¼ˆWeek 6-8ï¼‰

æ•°å­¦çš„äºˆæ¸¬ã‚’**å‰æ**ã¨ã—ã¦ã€æœ‰æ©Ÿçš„åˆ†æï¼ˆLLMï¼‰ã‚’è¿½åŠ :

ã€5ã‚¹ãƒ†ãƒƒãƒ—å¼·åˆ¶ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã€‘
1. æ•°å­¦çš„åˆ†æçµæœã®ç¢ºèªï¼ˆå¿…é ˆï¼‰
   - GRUäºˆæ¸¬: +3.2%
   - GARCHäºˆæ¸¬: 2.8%
   - ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ä¿¡é ¼åº¦: 85%

2. ãƒ‹ãƒ¥ãƒ¼ã‚¹å½±éŸ¿ã®å®šé‡åŒ–
   Q: ã“ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã§GRUäºˆæ¸¬+3.2%ã¯ã©ã†å¤‰ã‚ã‚‹ï¼Ÿ
   â†’ ä¿®æ­£äºˆæ¸¬: +?%ï¼ˆæ•°å€¤ã§ç­”ãˆã‚‹ï¼‰

3. æ•°å­¦ã§æ‰ãˆã‚‰ã‚Œãªã„è¦å› 
   - è¦åˆ¶ãƒªã‚¹ã‚¯
   - å¸‚å ´ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆ
   - ãƒã‚¯ãƒ­çµŒæ¸ˆ

4. ç·åˆè©•ä¾¡
   æ•°å­¦: +3.2%
   æœ‰æ©Ÿ: +1.5%ï¼ˆãƒ‹ãƒ¥ãƒ¼ã‚¹è£œæ­£ï¼‰
   â†’ æœ€çµ‚äºˆæ¸¬: +2.8%

5. æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³
   BUY / SELL / HOLDï¼ˆç†ç”±ä»˜ãï¼‰
```

**ãƒ¦ã‚¦ã‚¿**: ã€Œæ•°å­¦ãŒåŸºç›¤ã§ã€æœ‰æ©Ÿçš„åˆ†æãŒãã®ä¸Šã«ä¹—ã‚‹ã‚“ã ãªã€

**ãƒŸã‚³**: ã€Œãã†ã€‚**åœŸå°ï¼ˆæ•°å­¦ï¼‰ â†’ å£ï¼ˆçµ±åˆï¼‰ â†’ å±‹æ ¹ï¼ˆæœ‰æ©Ÿï¼‰**ã®é †ç•ªã ã€

---

## Scene 11: å®Ÿè£…ã‚³ãƒ¼ãƒ‰ï¼ˆå®Œå…¨ç‰ˆï¼‰

**ãƒŸã‚³**: ã€Œå®Ÿè£…ã®å…¨ä½“åƒã‚’è¦‹ã›ã‚ˆã†ã€

```python
# src/analysis/gru_forecaster.pyï¼ˆå®Œå…¨ç‰ˆï¼‰

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler


class GRUForecastingEngine:
    """GRUäºˆæ¸¬ã‚¨ãƒ³ã‚¸ãƒ³"""

    def __init__(self, lookback=60, forecast_horizon=7, hidden_size=50, num_layers=2):
        """
        Args:
            lookback: éå»ä½•æ—¥åˆ†ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ã†ã‹ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 60æ—¥ï¼‰
            forecast_horizon: ä½•æ—¥å…ˆã‚’äºˆæ¸¬ã™ã‚‹ã‹ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 7æ—¥ï¼‰
            hidden_size: GRUã®éš ã‚Œå±¤æ¬¡å…ƒ
            num_layers: GRUå±¤ã®æ•°
        """
        self.lookback = lookback
        self.forecast_horizon = forecast_horizon
        self.hidden_size = hidden_size
        self.num_layers = num_layers

        self.model = None
        self.scaler = MinMaxScaler(feature_range=(0, 1))
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    def prepare_data(self, df: pd.DataFrame, train_ratio=0.8):
        """æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã‚’å­¦ç¿’ç”¨ã«å¤‰æ›"""
        # æ­£è¦åŒ–ï¼ˆ0-1ã®ç¯„å›²ã«ï¼‰
        data = df[['open', 'high', 'low', 'close', 'volume']].values
        data_normalized = self.scaler.fit_transform(data)

        # ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚’ä½œæˆ
        X, y = [], []
        for i in range(len(data_normalized) - self.lookback - self.forecast_horizon + 1):
            X.append(data_normalized[i:i + self.lookback])
            target_idx = i + self.lookback + self.forecast_horizon - 1
            y.append(data_normalized[target_idx, 3])  # closeä¾¡æ ¼

        X = np.array(X)
        y = np.array(y).reshape(-1, 1)

        # è¨“ç·´/æ¤œè¨¼/ãƒ†ã‚¹ãƒˆåˆ†å‰²
        train_size = int(len(X) * train_ratio)
        val_size = int(len(X) * 0.1)

        X_train, y_train = X[:train_size], y[:train_size]
        X_val, y_val = X[train_size:train_size + val_size], y[train_size:train_size + val_size]
        X_test, y_test = X[train_size + val_size:], y[train_size + val_size:]

        # DataLoaderã‚’ä½œæˆ
        train_loader = DataLoader(TimeSeriesDataset(X_train, y_train), batch_size=32, shuffle=True)
        val_loader = DataLoader(TimeSeriesDataset(X_val, y_val), batch_size=32, shuffle=False)

        return train_loader, val_loader, (X_test, y_test)

    def train(self, train_loader, val_loader, epochs=100, learning_rate=0.001):
        """ãƒ¢ãƒ‡ãƒ«è¨“ç·´"""
        self.model = GRUModel(
            input_size=5,
            hidden_size=self.hidden_size,
            num_layers=self.num_layers
        ).to(self.device)

        criterion = nn.MSELoss()
        optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)

        best_val_loss = float('inf')
        patience = 10
        patience_counter = 0

        for epoch in range(epochs):
            # è¨“ç·´
            self.model.train()
            train_loss = 0.0
            for X_batch, y_batch in train_loader:
                X_batch = X_batch.to(self.device)
                y_batch = y_batch.to(self.device)

                outputs = self.model(X_batch)
                loss = criterion(outputs, y_batch)

                optimizer.zero_grad()
                loss.backward()
                optimizer.step()

                train_loss += loss.item()

            train_loss /= len(train_loader)

            # æ¤œè¨¼
            self.model.eval()
            val_loss = 0.0
            with torch.no_grad():
                for X_batch, y_batch in val_loader:
                    X_batch = X_batch.to(self.device)
                    y_batch = y_batch.to(self.device)
                    outputs = self.model(X_batch)
                    loss = criterion(outputs, y_batch)
                    val_loss += loss.item()

            val_loss /= len(val_loader)

            # Early Stopping
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                patience_counter = 0
                torch.save(self.model.state_dict(), 'best_gru_model.pth')
            else:
                patience_counter += 1

            if patience_counter >= patience:
                break

        self.model.load_state_dict(torch.load('best_gru_model.pth', weights_only=True))

    def forecast(self, df: pd.DataFrame, periods=7):
        """äºˆæ¸¬å®Ÿè¡Œ"""
        recent_data = df[['open', 'high', 'low', 'close', 'volume']].tail(self.lookback).values
        recent_data_normalized = self.scaler.transform(recent_data)

        forecasts = []
        current_input = recent_data_normalized.copy()

        self.model.eval()
        with torch.no_grad():
            for _ in range(periods):
                X = torch.FloatTensor(current_input).unsqueeze(0).to(self.device)
                pred = self.model(X).cpu().numpy()[0, 0]
                forecasts.append(pred)

                next_row = current_input[-1].copy()
                next_row[3] = pred
                current_input = np.vstack([current_input[1:], next_row])

        # é€†æ­£è¦åŒ–
        forecasts_denorm = self._denormalize_price(np.array(forecasts).reshape(-1, 1))

        current_price = df['close'].iloc[-1]
        final_forecast = forecasts_denorm[-1][0]
        forecast_change = ((final_forecast - current_price) / current_price) * 100

        return {
            'forecast': forecasts_denorm.flatten().tolist(),
            'current_price': float(current_price),
            'forecast_price': float(final_forecast),
            'forecast_change': float(forecast_change)
        }

    def _denormalize_price(self, normalized_value):
        """æ­£è¦åŒ–ã•ã‚ŒãŸä¾¡æ ¼ã‚’å…ƒã®ã‚¹ã‚±ãƒ¼ãƒ«ã«æˆ»ã™"""
        dummy = np.zeros((len(normalized_value), 5))
        dummy[:, 3] = normalized_value.flatten()
        denormalized = self.scaler.inverse_transform(dummy)
        return denormalized[:, 3].reshape(-1, 1)


class TimeSeriesDataset(Dataset):
    """æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ"""

    def __init__(self, X, y):
        self.X = torch.FloatTensor(X)
        self.y = torch.FloatTensor(y)

    def __len__(self):
        return len(self.X)

    def __getitem__(self, idx):
        return self.X[idx], self.y[idx]
```

**ãƒ¦ã‚¦ã‚¿**: ã€Œã™ã’ãˆ...ã“ã‚ŒãŒæœ€å…ˆç«¯ã‹ã€

**ãƒŸã‚³**: ã€Œã¾ã å§‹ã¾ã£ãŸã°ã‹ã‚Šã ã€‚ã“ã‚Œã‹ã‚‰æ¯å¹´ã€æ–°ã—ã„æ‰‹æ³•ãŒç”Ÿã¾ã‚Œã‚‹ã€

---

## Scene 12: ã‚¨ãƒ”ãƒ­ãƒ¼ã‚° - æ•°å­¦çš„åŸºç›¤ã®ç¢ºç«‹

**ãƒ¦ã‚¦ã‚¿**: ã€Œã“ã‚Œã§...ä¿ºãŸã¡ã®æ­¦å™¨ãŒæƒã£ãŸãªã€

**ãƒŸã‚³**: ã€Œæ•°å­¦çš„åŸºç›¤ã¯ç¢ºç«‹ã—ãŸã€

```markdown
ã€ç¿’å¾—ã—ãŸæ•°å­¦çš„æ­¦å™¨ã€‘

Phase 1: åŸºç¤ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«åˆ†æ
âœ… RSIï¼ˆè²·ã‚ã‚Œã™ã/å£²ã‚‰ã‚Œã™ãï¼‰
âœ… MACDï¼ˆãƒˆãƒ¬ãƒ³ãƒ‰æ–¹å‘ï¼‰
âœ… Bollinger Bandsï¼ˆãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ï¼‰

Phase 2: çµ±è¨ˆçš„äºˆæ¸¬
âœ… ARIMAï¼ˆä¾¡æ ¼äºˆæ¸¬ã€MAPE 2.15%ï¼‰
âœ… GARCHï¼ˆãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£äºˆæ¸¬ï¼‰

Phase 3: æ©Ÿæ¢°å­¦ç¿’ï¼ˆæœ€æ–°ï¼‰
âœ… GRUï¼ˆä¾¡æ ¼äºˆæ¸¬ã€MAPE 0.09%ã€24å€æ”¹å–„ï¼‰
âœ… LSTM-GARCHï¼ˆãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£äºˆæ¸¬ã€2.6å€æ”¹å–„ï¼‰
âœ… ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ï¼ˆè¤‡æ•°ãƒ¢ãƒ‡ãƒ«çµ±åˆï¼‰

ç·åˆç²¾åº¦: 10-20å€ã®å‘ä¸Šï¼ˆæœŸå¾…å€¤ï¼‰
```

**ãƒŸã‚³**: ã€Œã§ã‚‚ã€ã“ã‚Œã ã‘ã˜ã‚ƒã¾ã ã€æ™®é€šã®ãƒˆãƒ¬ãƒ¼ãƒ€ãƒ¼ã€ã¨åŒã˜ã ã€

**ãƒ¦ã‚¦ã‚¿**: ã€Œãˆï¼Ÿã€

**ãƒŸã‚³**: ã€Œæ•°å­¦ã ã‘ã§ã¯æ‰ãˆã‚‰ã‚Œãªã„ã€å®šæ€§çš„ãªè¦å› ã€ãŒã‚ã‚‹ã€

**ãƒŸã‚³**: ã€Œæ¬¡ã¯...ã€æœ‰æ©Ÿçš„åˆ†æã€ã ã€

**ãƒŸã‚³**: ã€Œæ•°å­¦çš„äºˆæ¸¬ã‚’å‰æã¨ã—ã¦ã€ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚„å¸‚å ´å¿ƒç†ã‚’çµ„ã¿è¾¼ã‚€ã€

**ãƒŸã‚³**: ã€Œãã‚ŒãŒã€ãƒ—ãƒ­ãƒ•ã‚§ãƒƒã‚·ãƒ§ãƒŠãƒ«ã¨ã‚¢ãƒãƒãƒ¥ã‚¢ã®é•ã„ã ã€

---

## ğŸ“ Chapter 7 ã¾ã¨ã‚

### é«˜åº¦ãªæ•°å­¦æ‰‹æ³•ï¼ˆ2024-2025æœ€æ–°ï¼‰

```markdown
ã€GRUï¼ˆGated Recurrent Unitï¼‰ã€‘
- ç™ºè¡¨: 2024å¹´ï¼ˆMDPIè«–æ–‡ï¼‰
- ç²¾åº¦: MAPE 0.09%ï¼ˆARIMAã®24å€ï¼‰
- ç”¨é€”: çŸ­æœŸã€œä¸­æœŸä¾¡æ ¼äºˆæ¸¬ï¼ˆ1-7æ—¥ï¼‰
- ç‰¹å¾´: éç·šå½¢ãƒ‘ã‚¿ãƒ¼ãƒ³ã€é•·æœŸä¾å­˜æ€§

ã€LSTM-GARCHãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã€‘
- ç™ºè¡¨: 2023å¹´ï¼ˆComputational Economicsï¼‰
- ç²¾åº¦: MSE 0.000034ï¼ˆGARCHã®2.6å€ï¼‰
- ç”¨é€”: ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£äºˆæ¸¬
- ç‰¹å¾´: ä¾¡æ ¼ã¨ãƒªã‚¹ã‚¯ã‚’åŒæ™‚äºˆæ¸¬

ã€ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ‰‹æ³•ã€‘
- ç²¾åº¦: å˜ä¸€ãƒ¢ãƒ‡ãƒ«ã‚ˆã‚Š1.5å€æ”¹å–„
- ç”¨é€”: å…¨ç›¸å ´å¯¾å¿œ
- ç‰¹å¾´: è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã®å¼±ç‚¹ã‚’è£œå®Œ

ã€ãƒ¦ã‚¦ã‚¿å¼ã§ã®ä½¿ã„æ–¹ã€‘
- æ•°å­¦çš„åŸºç›¤ã¨ã—ã¦æœ€å„ªå…ˆ
- æœ‰æ©Ÿçš„åˆ†æã®å‰æ
- ãƒªã‚¹ã‚¯ç®¡ç†ã®åŸºæº–
```

### å®Ÿè£…å®Œäº†

âœ… `src/analysis/gru_forecaster.py`
- `GRUModel`: GRUãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯
- `GRUForecastingEngine`: äºˆæ¸¬ã‚¨ãƒ³ã‚¸ãƒ³
- `prepare_data()`: ãƒ‡ãƒ¼ã‚¿æº–å‚™
- `train()`: ãƒ¢ãƒ‡ãƒ«è¨“ç·´
- `forecast()`: äºˆæ¸¬å®Ÿè¡Œ

---

## ğŸ› ï¸ å®Ÿè·µã§ä½¿ã†

é«˜åº¦ãªæ•°å­¦æ‰‹æ³•ã‚’å®Ÿéš›ã®å–å¼•ã§æ´»ç”¨ã™ã‚‹æ–¹æ³•ï¼š
- **Week 2: GRUå®Ÿè£…ã¨ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆ**ï¼ˆä½œæˆäºˆå®šï¼‰ - GRUã§24å€ã®ç²¾åº¦å‘ä¸Šã‚’ç¢ºèª
- **Week 3: ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ãƒ¢ãƒ‡ãƒ«å®Ÿè£…**ï¼ˆä½œæˆäºˆå®šï¼‰ - LSTM-GARCHã€ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«
- **Week 4: ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆã¨æœ€é©åŒ–**ï¼ˆä½œæˆäºˆå®šï¼‰ - éå»1å¹´ã§æ¤œè¨¼ã€æœ€é©ãªãƒ¢ãƒ‡ãƒ«é¸å®š

---

**Next Chapter**: [Chapter 8: æœ‰æ©Ÿçš„åˆ†æ - æ•°å­¦ã‚’è¶…ãˆã‚‹æ´å¯Ÿ](./08_organic_analysis.md)ï¼ˆä½œæˆäºˆå®šï¼‰

---

## ğŸ“– æ¬¡ã¸

æ¬¡ã¯**Week 2: GRUå®Ÿè£…**ã§ã€å®Ÿéš›ã«GRUã‚’ä½¿ã£ãŸå–å¼•ã‚·ã‚¹ãƒ†ãƒ ã‚’æ§‹ç¯‰ã—ã¾ã™ã€‚

---

## ğŸ“š å‚è€ƒæ–‡çŒ®

### ä¸»è¦è«–æ–‡

1. **"High-Frequency Cryptocurrency Price Forecasting Using Machine Learning Models"**
   - ç™ºè¡¨: MDPI, 2024
   - ä¸»ãªæˆæœ: GRU MAPE 0.09%ï¼ˆARIMA 2.15%ï¼‰

2. **"LSTMâ€“GARCH Hybrid Model for the Prediction of Volatility"**
   - ç™ºè¡¨: Computational Economics, 2023
   - ä¸»ãªæˆæœ: MSE 0.000034ï¼ˆGARCHå˜ç‹¬ 0.000089ï¼‰

3. **"Forecasting cryptocurrency volatility using evolving multiscale GNN"**
   - ç™ºè¡¨: Financial Innovation, 2025
   - ä¸»ãªæˆæœ: Graph Neural Networksã§å¸‚å ´é–“ç›¸é–¢ã‚’æ‰ãˆã‚‹

4. **"CryptoMamba: Leveraging State Space Models"**
   - ç™ºè¡¨: arXiv, 2025
   - ä¸»ãªæˆæœ: State Space Modelsï¼ˆMambaï¼‰ã§é•·æœŸä¾å­˜æ€§ã‚’åŠ¹ç‡çš„ã«å­¦ç¿’

### å­¦ç¿’ãƒªã‚½ãƒ¼ã‚¹

- PyTorchå…¬å¼ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«: https://pytorch.org/tutorials/
- Deep Learning Book (Goodfellow et al.): https://www.deeplearningbook.org/
- Time Series Forecasting with Deep Learning: https://machinelearningmastery.com/
